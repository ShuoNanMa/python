{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Machine Learning](05.00-Machine-Learning.ipynb) | [Contents](Index.ipynb) | [Introducing Scikit-Learn](05.02-Introducing-Scikit-Learn.ipynb) >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 什么是机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在介绍各种机器学习方法之前，先看看究竟什么是机器学习，什么不是机器学习。机器学习经常被归类为人工智能（artiﬁcial intelligence）的子领域，但我觉得这种归类方法存在误导嫌疑。虽然对机器学习的研究确实是源自人工智能领域，但是机器学习的方法却应用于数据科学领域，因此我认为把机器学习看作是一种**数学建模**更合适。\n",
    "\n",
    "机器学习的本质就是借助数学模型理解数据。当我们给模型装上可以适应观测数据的可调参数时，“学习”就开始了；此时的程序被认为具有从数据中“学习”的能力。一旦模型可以拟合旧的观测数据，那么它们就可以预测并解释新的观测数据。在后面的内容中，我会分享一些关于这种数学方法的哲学闲话，你会发现数学模型的“学习”过程其实与人脑的“学习”过程相似。\n",
    "\n",
    "由于理解机器学习问题的类型对于有效使用各种机器学习工具至关重要，因此首先介绍关于机器学习方法的若干分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 机器学习的分类\n",
    "\n",
    "机器学习一般可以分为两类：有监督学习（supervised learning）和无监督学习（unsupervised \n",
    "learning）。\n",
    "\n",
    "**有监督学习**是指对数据的若干**特征**与若干**标签（类型）**之间的关联性进行建模的过程；只要模型被确定，就可以应用到新的未知数据上。这类学习过程可以进一步分为**分类\n",
    "（classiﬁcation）任务**与**回归（regression）任务**。在分类任务中，标签都是**离散值**；而在回归任务中，标签都是**连续值**。我们会在后面的内容中介绍这两种有监督学习方法。\n",
    "\n",
    "**无监督学习**是指对**不带任何标签**的**数据特征**进行建模，通常被看成是一种“让数据自己介绍自己”的过程。这类模型包括**聚类（clustering）任务**和**降维（dimensionality reduction）任务**。聚类算法可以将数据分成不同的组别，而降维算法追求用更简洁的方式表现数据。我们同样会在后面的内容中介绍这两种无监督学习方法。\n",
    "\n",
    "另外，还有一种**半监督学习（semi-supervised learning）**方法，介于有监督学习与无监督学习之间。半监督学习方法通常可以在数据标签不完整时使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 机器学习应用的定性示例\n",
    "\n",
    "下面来介绍一些简单的机器学习任务示例，让这些抽象理论显得更具体一点。\n",
    "\n",
    "这些例子都是我们在后面内容中将要看到的机器学习任务的直观、非量化形式，之后将更深入地介绍\n",
    "相关模型的具体用法。如果想尽早了解这些技术的更多细节，那么请参见在线附录[Appendix: Figure Code](06.00-Figure-Code.ipynb)中生成下面各个示例中彩图的Python代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 分类：预测离散标签\n",
    "\n",
    "先来看一个简单的**分类**任务。假如我们有一些带标签的数据点，希望用这些信息为那些不带标签的数据点进行分类。\n",
    "\n",
    "假如这些数据点的分布如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-classification-1.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Classification-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到的是二维数据，也就是说每个数据点都有两个**特征**（features），在平面上用数据点的(x,y)位置表示（更明确的说：x是一个特征，y是一个特征）。另外，我们的数据点还用一种颜色表示一个**类型标签**（class labels），即红色和蓝色。我们想根据这些特征和标签创建一个模型，帮助我们判断新的数据点是“蓝色”还是“红色”。\n",
    "\n",
    "虽然有许多可以解决分类任务的模型，但是这里还是先用最简单的一种。假设平面上有一条可以将两种颜色分开的直线，直线的两侧分别是一种颜色。那么，我们的**模型**其实就是“一条可以分类的直线”，而**模型参数**其实就是直线位置与方向的数值。这些模型参数的最优解都可以通过学习数据获得（也就是机器学习的“学习”），这个过程通常被称为**训练模型**。\n",
    "\n",
    "下图是为这组数据分类而训练的模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-classification-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Classification-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this model has been trained, it can be generalized to new, unlabeled data.\n",
    "In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model.\n",
    "This stage is usually called *prediction*. See the following figure:\n",
    "模型现在已经训练好了，可以对一个新的、不带标签的数据进行分类了。也就是说，我们可以拿一组新数据，把这个模型的直线画在上面，然后根据这个模型为新数据分配标签。\n",
    "\n",
    "这个阶段通常被称为**预测**，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-classification-3.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Classification-Example-Figure-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是机器学习中最基本的分类思想，这个“分类”指的是数据具有离散的类型标签。刚一开始，你可能会觉得分类非常简单：不就是直接观察数据，然后画一条分割线就可以了。但是，机器学习方法的真正用途是要**解决大型高维度数据集的分类问题**。\n",
    "\n",
    "以常见的分类任务——垃圾邮件自动识别为例。在这类任务中，我们通常会获得以下特征与标签。\n",
    "\n",
    "- **特征 1、特征 2……特征 n** → 垃圾邮件关键词与短语出现的频次归一化向量（“Viagra”  “Nigerian prince”等）。\n",
    "\n",
    "- **标签** →  “垃圾邮件”或“普通邮件”。\n",
    "\n",
    "在训练数据集中，这些标签可能是人们通过观察少量邮件样本得到的，而剩下的大量邮件都需要通过模型来判断标签。一个训练有素的分类算法只要具备足够好的特征（通常是成千上万个词或短语），就能非常高效地进行分类。\n",
    "\n",
    "常用的分类算法包括高斯朴素贝叶斯分类、支持向量机以及随机森林分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归：预测连续标签\n",
    "\n",
    "回归任务与离散标签分类算法相反，其标签是**连续值**。\n",
    "\n",
    "观察下图所示的数据集，所有样本的标签，即点的颜色是渐变的，表明颜色值处在一个连续的区间内。\n",
    "\n",
    "和前面的分类示例一样，我们有一个二维数据，每个数据点有两个特征（x和y特征）。数据点的颜色表示每个点的连续标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-1.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Regression-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然有许多可以处理这类数据的回归模型，但是我们还是用简单线性回归模型来预测数据。用简单线性回归模型作出假设，如果我们把标签看成是第三个维度，那么就可以将数据拟合成一个平面方程——这就是著名的在二维平面上线性拟合问题的高阶情形。\n",
    "\n",
    "我们可以将数据可视化成下图的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Regression-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，上图中的特征1与特征2平面与之前的二维图形是一样的，只不过用了颜色和三维坐标轴的位置表示标签。通过这个视角，就有理由相信：如果将三维数据拟合成一个平面，就可以对任何输入参数集进行预测。回到原来的二维投影图形上，拟合平面时获得的结果如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-3.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Regression-Example-Figure-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个拟合平面为预测新数据点的标签提供了依据。我们可以直观地找到结果，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-regression-4.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Regression-Example-Figure-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和之前介绍的分类示例类似，这个回归示例在低维度时看起来可能也非常简单。但是这些方法的真实价值在于，它们可以直截了当地处理包含大量特征的数据集。\n",
    "\n",
    "类似的任务有计算通过天文望远镜观测到的星系的距离——在这类任务中，可能会用到以下特征与标签。\n",
    "- **特征1、特征2……特征n** → 具有若干波长或颜色的星系的亮度。\n",
    "\n",
    "- **标签** → 星系的距离或红移（redshift）。\n",
    "\n",
    "少量星系的距离可以通过直接观察（通常成本也非常高）进行测量。之后，我们就可以利用适当的回归模型估计其他星系的距离，而不需要为整个星系集合使用昂贵的观察设备。\n",
    "\n",
    "在天文学领域中，这种问题通常被称为“测光红移”（photometric redshift）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  聚类：推测无标签数据的标签\n",
    "\n",
    "前面介绍的回归与分类示例都是有监督学习算法，需要建立一个模型来预测新数据的标签。无监督学习涉及的模型将探索没有任何已知标签的数据。\n",
    "\n",
    "无监督学习的普遍应用之一就是**“聚类”**——数据被聚类算法自动分成若干离散的组别。\n",
    "\n",
    "例如，我们有如下图所示的一组二维数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-clustering-1.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅通过肉眼观察，就可以很清晰地判断出这些点应该归于哪个组。\n",
    "\n",
    "一个聚类模型会根据输入数据的固有结构判断数据点之间的相关性。通过最快、最直观的k-means 聚类算法（详情请参见[In Depth: K-Means Clustering](05.11-K-Means.ipynb)） ，就可以发现如下图所示的类簇（cluster）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-clustering-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Clustering-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*k*-means会拟合出一个由*k*个簇中心点构成的模型，最优的簇中心点需要满足簇中的每个点到该中心的总距离最短。显然，在二维平面上用聚类算法显得非常幼稚，但随着数据量越来越大、维度越来越多，聚类算法对于探索数据集的信息会变得十分有效。\n",
    "\n",
    "我们将在[In Depth: K-Means Clustering](05.11-K-Means.ipynb)详细介绍*k*-means聚类算法。其他重要的聚类算法还有高斯混合模型（详情请参见[In Depth: Gaussian Mixture Models](05.12-Gaussian-Mixtures.ipynb)）和谱聚类（详情请参考Scikit-Learn聚类文档，[Scikit-Learn's clustering documentation](http://scikit-learn.org/stable/modules/clustering.html)）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维：推断无标签数据的结构\n",
    "\n",
    "降维是另一种无监督算法示例，需要从数据集本身的结构推断标签和其他信息。虽然降维比之前看到的示例要抽象一些，但是一般来说，降维其实就是在保证高维数据质量的条件下从中抽取出一个低维数据集。不同的降维算法用不同的方式衡量降维质量，[In-Depth: Manifold Learning](05.10-Manifold-Learning.ipynb)将介绍这些内容。\n",
    "\n",
    "下面用一个示例进行演示，数据如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-dimesionality-1.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以清晰地看出数据存在某种结构：这些数据点在二维平面上按照一维螺旋线整齐地排列。\n",
    "\n",
    "从某种程度上，你可以说这些数据“本质上”只有一维，虽然这个一维数据是嵌在高维数据空间里的。适合这个示例的降维模型不仅需要满足数据的非线性嵌套结构，而且还要给出低维表现形式。\n",
    "\n",
    "下图是通过Isomap算法得到的可视化结果，它是一种专门用于解决这类问题的流形学习算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/05.01-dimesionality-2.png)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Dimensionality-Reduction-Example-Figure-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，图中的颜色（表示算法提取到的一维潜在变量）沿着螺旋线呈现均匀变化，表明这个算法的确发现了肉眼所能观察到的结构。和之前介绍的示例类似，降维算法同样要在处理高维数据时才能大展拳脚。\n",
    "\n",
    "例如，我们可能需要对一个包含100或1000个特征的数据集内部的关联性进行可视化。要对一个1000维的数据进行可视化是个巨大的挑战，一种解决办法就是通过降维技术，让我们可以在更容易处理的二维或三维空间中对数据进行可视化。\n",
    "\n",
    "我们还会详细介绍一些重要的降维算法，包括主成分分析（详情请参见[In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)）和各种流形学习算法，如Isomap算法、局部线性嵌入算法（详情请参见[In-Depth: Manifold Learning](05.10-Manifold-Learning.ipynb)）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 小结\n",
    "\n",
    "前面介绍了一些机器学习方法基本类型的示例。虽然我们略过了许多重要的实践细节，但我还是希望这节的内容可以让你对用机器学习方法解决问题的基本思路有所了解。\n",
    "\n",
    "综上所述，本节介绍的主要有以下内容。\n",
    "\n",
    "- **有监督学习**：可以训练带标签的数据以预测新数据标签的模型。\n",
    "\n",
    "  - *分类*：可以预测两个或多个离散分类标签的模型。\n",
    " \n",
    "  - *回归*：可以预测连续标签的模型。\n",
    "  \n",
    "- **无监督学习**：识别无标签数据结构的模型。\n",
    "\n",
    "- **聚类**：检测、识别数据显著组别的模型。\n",
    "\n",
    "- **降维**：从高维数据中检测、识别低维数据结构的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Machine Learning](05.00-Machine-Learning.ipynb) | [Contents](Index.ipynb) | [Introducing Scikit-Learn](05.02-Introducing-Scikit-Learn.ipynb) >"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
